{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Process\n",
        "## Policy Iteration method\n",
        "### Step 1 :- Applying the grid world"
      ],
      "metadata": {
        "id": "HMY5TDGVCNtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "N2KnR-pCEdG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self, n, m):\n",
        "        self.n = n  # Number of rows\n",
        "        self.m = m  # Number of columns\n",
        "        self.state = (0, 0)  # Agent's initial position (x, y)\n",
        "        self.actions = ['Up', 'Down', 'Left', 'Right']\n",
        "        self.grid = self.grid_world()  # Initialize the grid\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "\n",
        "        # Calculate the new position based on the action\n",
        "        if action == 'Up':\n",
        "            x_new, y_new = x - 1, y\n",
        "        elif action == 'Down':\n",
        "            x_new, y_new = x + 1, y\n",
        "        elif action == 'Left':\n",
        "            x_new, y_new = x, y - 1\n",
        "        elif action == 'Right':\n",
        "            x_new, y_new = x, y + 1\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        # Check if the new position is valid (within bounds and not an obstacle)\n",
        "        if 0 <= x_new < self.n and 0 <= y_new < self.m and self.grid[x_new, y_new] != -1:\n",
        "            self.state = (x_new, y_new)\n",
        "\n",
        "        # Calculate reward\n",
        "        if self.grid[x_new, y_new] == 2:  # Goal state\n",
        "            reward = 10\n",
        "            done = True\n",
        "        elif self.grid[x_new, y_new] == -1:  # Obstacle\n",
        "            reward = -10\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)  # Reset to the start state\n",
        "        return self.state\n",
        "\n",
        "    def render(self):\n",
        "        grid = self.grid.copy()\n",
        "        x, y = self.state\n",
        "        grid[x, y] = 9\n",
        "        print(grid)\n",
        "\n",
        "    def grid_world(self):\n",
        "        grid = np.zeros((self.n, self.m), dtype=int)\n",
        "        start_state = (0, 0)\n",
        "        goal_state = (self.n - 1, self.m - 1)\n",
        "        obstacles = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 0), (3, 1), (3, 2)]\n",
        "\n",
        "        grid[start_state] = 1  # Start state\n",
        "        grid[goal_state] = 2  # Goal state\n",
        "        for obstacle in obstacles:\n",
        "            grid[obstacle] = -1  # Obstacles\n",
        "\n",
        "        return grid\n",
        "\n",
        "    def random_policy(self):\n",
        "        return random.choice(self.actions)\n",
        "\n",
        "    def value(self):\n",
        "        value_table = np.zeros((self.n, self.m))\n",
        "\n",
        "\n",
        "    def value_iteration(self, gamma=0.9, theta=1e-6):\n",
        "        \"\"\"\n",
        "        Value Iteration Algorithm to compute optimal state values.\n",
        "        \"\"\"\n",
        "        V = np.zeros((self.n, self.m))  # Initialize state values\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for x in range(self.n):\n",
        "                for y in range(self.m):\n",
        "                    if self.grid[x, y] in [-1, 2]:  # Skip obstacles & goal\n",
        "                        continue\n",
        "\n",
        "                    v = V[x, y]\n",
        "                    values = []\n",
        "                    for action in self.actions:\n",
        "                        x_new, y_new = self.next_position(x, y, action)\n",
        "                        reward = self.get_reward(x_new, y_new)\n",
        "                        values.append(reward + gamma * V[x_new, y_new])\n",
        "\n",
        "                    V[x, y] = max(values)\n",
        "                    delta = max(delta, abs(v - V[x, y]))\n",
        "\n",
        "            if delta < theta:\n",
        "                break\n",
        "        return V\n",
        "\n",
        "    def policy_iteration(self, gamma=0.9, theta=1e-6):\n",
        "        \"\"\"\n",
        "        Policy Iteration Algorithm: Finds an optimal policy.\n",
        "        \"\"\"\n",
        "        policy = np.random.choice(self.actions, size=(self.n, self.m))\n",
        "        V = np.zeros((self.n, self.m))\n",
        "\n",
        "        while True:\n",
        "            # Policy Evaluation\n",
        "            while True:\n",
        "                delta = 0\n",
        "                for x in range(self.n):\n",
        "                    for y in range(self.m):\n",
        "                        if self.grid[x, y] in [-1, 2]:  # Skip obstacles & goal\n",
        "                            continue\n",
        "\n",
        "                        v = V[x, y]\n",
        "                        action = policy[x, y]\n",
        "                        x_new, y_new = self.next_position(x, y, action)\n",
        "                        reward = self.get_reward(x_new, y_new)\n",
        "                        V[x, y] = reward + gamma * V[x_new, y_new]\n",
        "                        delta = max(delta, abs(v - V[x, y]))\n",
        "\n",
        "                if delta < theta:\n",
        "                    break\n",
        "\n",
        "            # Policy Improvement\n",
        "            policy_stable = True\n",
        "            for x in range(self.n):\n",
        "                for y in range(self.m):\n",
        "                    if self.grid[x, y] in [-1, 2]:  # Skip obstacles & goal\n",
        "                        continue\n",
        "\n",
        "                    old_action = policy[x, y]\n",
        "                    values = {}\n",
        "                    for action in self.actions:\n",
        "                        x_new, y_new = self.next_position(x, y, action)\n",
        "                        reward = self.get_reward(x_new, y_new)\n",
        "                        values[action] = reward + gamma * V[x_new, y_new]\n",
        "\n",
        "                    policy[x, y] = max(values, key=values.get)\n",
        "                    if old_action != policy[x, y]:\n",
        "                        policy_stable = False\n",
        "\n",
        "            if policy_stable:\n",
        "                break\n",
        "\n",
        "        return policy, V\n",
        "\n",
        "    def next_position(self, x, y, action):\n",
        "        \"\"\"\n",
        "        Returns the next position based on action.\n",
        "        \"\"\"\n",
        "        if action == 'Up' and x > 0:\n",
        "            x -= 1\n",
        "        elif action == 'Down' and x < self.n - 1:\n",
        "            x += 1\n",
        "        elif action == 'Left' and y > 0:\n",
        "            y -= 1\n",
        "        elif action == 'Right' and y < self.m - 1:\n",
        "            y += 1\n",
        "        return x, y\n",
        "\n",
        "    def get_reward(self, x, y):\n",
        "        \"\"\"\n",
        "        Returns reward based on grid position.\n",
        "        \"\"\"\n",
        "        if self.grid[x, y] == 2:  # Goal\n",
        "            return 10\n",
        "        elif self.grid[x, y] == -1:  # Obstacle\n",
        "            return -10\n",
        "        return 0  # Default reward\n"
      ],
      "metadata": {
        "id": "gqh86hh0Ca25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(5,5)"
      ],
      "metadata": {
        "id": "kXt85bImDI53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.grid_world()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWwqj1TcDPUp",
        "outputId": "4120bf15-1f0d-4cda-f1cc-03b5d1a5eabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1, -1, -1,  0,  0],\n",
              "       [ 0,  0, -1, -1,  0],\n",
              "       [-1,  0,  0,  0,  0],\n",
              "       [ 0, -1, -1,  0,  0],\n",
              "       [ 0,  0,  0,  0,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy, values = env.policy_iteration()\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy)\n",
        "print(\"State Values:\")\n",
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh_x0zUDDsGH",
        "outputId": "32385ecc-2fa2-4b54-cfcf-cd87c3dac442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[['Down' 'Right' 'Down' 'Right' 'Down']\n",
            " ['Right' 'Down' 'Down' 'Left' 'Down']\n",
            " ['Down' 'Right' 'Right' 'Down' 'Down']\n",
            " ['Down' 'Up' 'Down' 'Down' 'Down']\n",
            " ['Right' 'Right' 'Right' 'Right' 'Left']]\n",
            "State Values:\n",
            "[[ 4.782969  0.        0.        6.561     7.29    ]\n",
            " [ 5.31441   5.9049    0.        0.        8.1     ]\n",
            " [ 0.        6.561     7.29      8.1       9.      ]\n",
            " [ 6.561     0.        0.        9.       10.      ]\n",
            " [ 7.29      8.1       9.       10.        0.      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r3iqQhXH0NG",
        "outputId": "d16aed5f-c20d-4bf8-b170-a38938a89946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 9 -1 -1  0  0]\n",
            " [ 0  0 -1 -1  0]\n",
            " [-1  0  0  0  0]\n",
            " [ 0 -1 -1  0  0]\n",
            " [ 0  0  0  0  2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from queue import Queue\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, n, m):\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.state = (0, 0)\n",
        "        self.actions = ['Up', 'Down', 'Left', 'Right']\n",
        "        self.grid = self.generate_valid_grid()  # Randomized grid with a valid path\n",
        "        self.path = []  # To store the optimal path\n",
        "\n",
        "    def generate_valid_grid(self):\n",
        "        \"\"\"\n",
        "        Generates a random grid where obstacles are placed such that there is always a valid path.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            grid = np.zeros((self.n, self.m), dtype=int)\n",
        "            grid[self.n - 1, self.m - 1] = 2  # Goal state\n",
        "\n",
        "            # Randomly place obstacles (ensuring start and goal are not blocked)\n",
        "            for _ in range(int(0.2 * self.n * self.m)):  # 20% of cells as obstacles\n",
        "                x, y = random.randint(0, self.n - 1), random.randint(0, self.m - 1)\n",
        "                if (x, y) != (0, 0) and (x, y) != (self.n - 1, self.m - 1):\n",
        "                    grid[x, y] = -1  # Obstacle\n",
        "\n",
        "            # Check if there's a valid path from (0,0) to (n-1,m-1)\n",
        "            if self.is_valid_path(grid):\n",
        "                return grid  # Return only if a path exists\n",
        "\n",
        "    def is_valid_path(self, grid):\n",
        "        \"\"\"\n",
        "        Uses BFS to check if there is a path from start (0,0) to goal (n-1, m-1).\n",
        "        \"\"\"\n",
        "        q = Queue()\n",
        "        q.put((0, 0))\n",
        "        visited = set()\n",
        "        visited.add((0, 0))\n",
        "\n",
        "        while not q.empty():\n",
        "            x, y = q.get()\n",
        "            if (x, y) == (self.n - 1, self.m - 1):\n",
        "                return True  # Path found\n",
        "\n",
        "            for action in self.actions:\n",
        "                nx, ny = self.next_position(x, y, action)\n",
        "                if (0 <= nx < self.n and 0 <= ny < self.m and\n",
        "                        grid[nx, ny] != -1 and (nx, ny) not in visited):\n",
        "                    q.put((nx, ny))\n",
        "                    visited.add((nx, ny))\n",
        "\n",
        "        return False  # No path found\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        x_new, y_new = self.next_position(x, y, action)\n",
        "\n",
        "        # Check if the new position is within bounds\n",
        "        if 0 <= x_new < self.n and 0 <= y_new < self.m:\n",
        "            if self.grid[x_new, y_new] != -1:  # Not an obstacle\n",
        "                self.state = (x_new, y_new)  # Move the agent\n",
        "                self.path.append((x_new, y_new))\n",
        "\n",
        "                if self.grid[x_new, y_new] == 2:  # Goal reached\n",
        "                    return self.state, 10, True  # Reward, Done = True\n",
        "\n",
        "                return self.state, 0, False  # Reward, Done = False\n",
        "\n",
        "        return self.state, -1, False  # Hitting a wall gives a small penalty\n",
        "\n",
        "    def next_position(self, x, y, action):\n",
        "        \"\"\"\n",
        "        Determines the next position given an action.\n",
        "        \"\"\"\n",
        "        if action == 'Up':\n",
        "            return x - 1, y\n",
        "        elif action == 'Down':\n",
        "            return x + 1, y\n",
        "        elif action == 'Left':\n",
        "            return x, y - 1\n",
        "        elif action == 'Right':\n",
        "            return x, y + 1\n",
        "        return x, y  # No movement if invalid action\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Displays the grid using textual symbols.\n",
        "        \"\"\"\n",
        "        display_grid = np.full((self.n, self.m), '.', dtype=str)\n",
        "\n",
        "        # Mark obstacles\n",
        "        for x in range(self.n):\n",
        "            for y in range(self.m):\n",
        "                if self.grid[x, y] == -1:\n",
        "                    display_grid[x, y] = 'ðŸš§'\n",
        "                elif self.grid[x, y] == 2:\n",
        "                    display_grid[x, y] = 'ðŸ'\n",
        "\n",
        "        # Mark the path taken\n",
        "        for x, y in self.path:\n",
        "            display_grid[x, y] = 'ðŸŸ©'\n",
        "\n",
        "        # Mark the agent\n",
        "        x, y = self.state\n",
        "        display_grid[x, y] = 'ðŸ¤–'\n",
        "\n",
        "        # Print the grid\n",
        "        for row in display_grid:\n",
        "            print(\" \".join(row))\n",
        "        print(\"\\n\")\n",
        "\n",
        "    def find_path(self):\n",
        "        \"\"\"\n",
        "        Finds an optimal path using BFS.\n",
        "        \"\"\"\n",
        "        q = Queue()\n",
        "        q.put((0, 0, []))  # Store the path in the queue\n",
        "        visited = set()\n",
        "        visited.add((0, 0))\n",
        "\n",
        "        while not q.empty():\n",
        "            x, y, path = q.get()\n",
        "            path.append((x, y))\n",
        "\n",
        "            if (x, y) == (self.n - 1, self.m - 1):\n",
        "                self.path = path  # Store the final path\n",
        "                return path  # Return the optimal path\n",
        "\n",
        "            for action in self.actions:\n",
        "                nx, ny = self.next_position(x, y, action)\n",
        "                if (0 <= nx < self.n and 0 <= ny < self.m and\n",
        "                        self.grid[nx, ny] != -1 and (nx, ny) not in visited):\n",
        "                    q.put((nx, ny, path[:]))  # Store path progression\n",
        "                    visited.add((nx, ny))\n",
        "\n",
        "        return []  # No path found\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Runs an agent to reach the goal using BFS path-finding.\n",
        "        \"\"\"\n",
        "        path = self.find_path()\n",
        "        for pos in path:\n",
        "            self.state = pos\n",
        "            self.render()  # Show the movement of the agent\n",
        "            if pos == (self.n - 1, self.m - 1):\n",
        "                print(\"ðŸŽ‰ Goal Reached! ðŸŽ‰\\n\")\n",
        "                break\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "env = Environment(6, 6)  # Create a 6x6 environment\n",
        "env.render()  # Display the initial state\n",
        "env.run()  # Run the agent to the goal\n"
      ],
      "metadata": {
        "id": "jL7WggGVgfcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}